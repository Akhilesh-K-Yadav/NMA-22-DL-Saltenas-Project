{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwbgwFC8L8Hm",
        "outputId": "707564d4-e00a-48d3-b3b0-eea3c14d0d5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/mindscope_utilities/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os, requests\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "#import scipy as sp\n",
        "# make some small edit\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "#from IPython.display import display\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Imports\n",
        "sns.set_context('notebook', font_scale=1.5, rc={'lines.markeredgewidth': 2})\n",
        "\n",
        "# Figure settings\n",
        "#import ipywidgets as widgets # Interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")\n",
        "\n",
        "# Set device\n",
        "DEVICE = 'cuda' #'cpu'\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "  import google.colab\n",
        "  google.colab.drive.mount('/content/drive')\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yof1b2Y1NL9p"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness. NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjMaI5M7OAFp"
      },
      "outputs": [],
      "source": [
        "# Retrieve the preprocessed data set (small)\n",
        "retrieve_preprocessed = False\n",
        "if retrieve_preprocessed:\n",
        "  fname = \"allen_visual_behavior_2p_change_detection_familiar_novel_image_sets.parquet\"\n",
        "  url = \"https://ndownloader.figshare.com/files/28470255\"\n",
        "\n",
        "  if not os.path.isfile(fname):\n",
        "    try:\n",
        "      r = requests.get(url)\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(fname, \"wb\") as fid:\n",
        "          fid.write(r.content)\n",
        "\n",
        "  filename = \"allen_visual_behavior_2p_change_detection_familiar_novel_image_sets.parquet\"\n",
        "  data = pd.read_parquet(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BIBlKolOcWO"
      },
      "outputs": [],
      "source": [
        "if retrieve_preprocessed:\n",
        "  sst_data = data[(data.cre_line == 'Sst-IRES-Cre')&(data.is_change == True)]\n",
        "  vip_data = data[(data.cre_line == 'Vip-IRES-Cre')&(data.is_change == True)]\n",
        "  timestamps = sst_data.trace_timestamps.values[0]\n",
        "  for exposure_level in sst_data.exposure_level.unique():\n",
        "    traces = sst_data[sst_data.exposure_level==exposure_level].trace.values\n",
        "    plt.plot(timestamps, np.mean(traces), label=exposure_level)\n",
        "  plt.title('SST population average')\n",
        "  plt.xlabel('time after change (sec)')\n",
        "  plt.ylabel('dF/F')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty6kfVFlO0YP"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "if retrieve_preprocessed:\n",
        "  # Select cells in multiple sessions\n",
        "  def get_cells_in_multiple_sessions(df):\n",
        "    cells_in_multiple_sessions = []\n",
        "    for cell_specimen_id in df.cell_specimen_id.unique():\n",
        "      if len(df[df.cell_specimen_id == cell_specimen_id].ophys_session_id.unique()) > 1:\n",
        "        cells_in_multiple_sessions.append(cell_specimen_id)\n",
        "    return cells_in_multiple_sessions\n",
        "\n",
        "  example_cell_specimen_id = vip_data[vip_data.exposure_level=='novel'].cell_specimen_id.unique()[7]\n",
        "  cell_data = vip_data[vip_data.cell_specimen_id == example_cell_specimen_id]\n",
        "  cell_data = cell_data[(cell_data.image_name == cell_data.image_name.unique()[2])]\n",
        "\n",
        "  # Pick a cell from a novel image session\n",
        "  example_cell_specimen_id = vip_data[vip_data.exposure_level=='novel'].cell_specimen_id.unique()[7]\n",
        "\n",
        "  cell_data = vip_data[vip_data.cell_specimen_id == example_cell_specimen_id]\n",
        "  cell_data = cell_data[(cell_data.image_name == cell_data.image_name.unique()[2])]\n",
        "\n",
        "  stimulus_presentations_id = cell_data.stimulus_presentations_id.unique()[0]\n",
        "  trial_data = cell_data[cell_data.stimulus_presentations_id == stimulus_presentations_id]\n",
        "  timestamps = trial_data.trace_timestamps.values[0]\n",
        "  trace = trial_data.trace.values[0]\n",
        "\n",
        "  color = [0, 0, 0]\n",
        "  plt.plot(timestamps,trace, color = color)\n",
        "\n",
        "  # Average over window\n",
        "  evk_window = np.logical_and(timestamps>0,timestamps<.6)\n",
        "  bkg_window = np.logical_and(timestamps<-.05,timestamps>-.15)\n",
        "\n",
        "  cell_mean = sst_data.apply(lambda x: x.trace[evk_window].mean()-x.trace[bkg_window].mean(),axis=1)\n",
        "\n",
        "  X_test, y_test, X_train, y_train = shuffle_and_split_data(cell_mean.values, sst_data.image_index.values,seed=SEED)\n",
        "\n",
        "  M = LogisticRegression().fit(X_train.reshape(-1, 1),y_train.reshape(-1, 1))\n",
        "  y_pred = M.predict(X_test.reshape(-1, 1))\n",
        "  score = accuracy_score(y_test.reshape(-1, 1),y_pred)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.subplot()\n",
        "  plt.scatter(y_test.reshape(-1,1)[0:300],y_pred[0:300])\n",
        "  print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1WWphObgMH5"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  \"\"\"\n",
        "  Initialize MLP Network\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, actv, input_feature_num, hidden_unit_nums, output_feature_num):\n",
        "    \"\"\"\n",
        "    Initialize MLP Network parameters\n",
        "\n",
        "    Args:\n",
        "      actv: string\n",
        "        Activation function\n",
        "      input_feature_num: int\n",
        "        Number of input features\n",
        "      hidden_unit_nums: int\n",
        "        Number of units in the hidden layer\n",
        "      output_feature_num: int\n",
        "        Number of output features\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(Net, self).__init__()\n",
        "    self.input_feature_num = input_feature_num # Save the input size for reshaping later\n",
        "    self.mlp = nn.Sequential() # Initialize layers of MLP\n",
        "\n",
        "    in_num = input_feature_num # Initialize the temporary input feature to each layer\n",
        "    for i in range(len(hidden_unit_nums)): # Loop over layers and create each one\n",
        "      out_num = hidden_unit_nums[i] # Assign the current layer hidden unit from list\n",
        "      layer = nn.Linear(in_num,out_num) # Use nn.Linear to define the layer\n",
        "      in_num = out_num # Assign next layer input using current layer output\n",
        "      self.mlp.add_module('Linear_%d'%i, layer) # Append layer to the model with a name\n",
        "\n",
        "      actv_layer = eval('nn.%s'%actv) # Assign activation function (eval allows us to instantiate object from string)\n",
        "      self.mlp.add_module('Activation_%d'%i, actv_layer) # Append activation to the model with a name\n",
        "\n",
        "    out_layer = nn.Linear(in_num, output_feature_num) # Create final layer\n",
        "    self.mlp.add_module('Output_Linear', out_layer) # Append the final layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Simulate forward pass of MLP Network\n",
        "\n",
        "    Args:\n",
        "      x: torch.tensor\n",
        "        Input data\n",
        "\n",
        "    Returns:\n",
        "      logits: Instance of MLP\n",
        "        Forward pass of MLP\n",
        "    \"\"\"\n",
        "    # Reshape inputs to (batch_size, input_feature_num)\n",
        "    # Just in case the input vector is not 2D, like an image!\n",
        "    x = x.view(-1, self.input_feature_num)\n",
        "    logits = self.mlp(x) # Forward pass of MLP\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWfITghLgfcC"
      },
      "outputs": [],
      "source": [
        "#Data selection and prep\n",
        "if 'cell_mean' not in vip_data.columns:\n",
        "  vip_data['cell_mean'] = vip_data.apply(lambda x: x.trace[evk_window].mean()-x.trace[bkg_window].mean(),axis=1)\n",
        "\n",
        "data_input_full = torch.zeros(len(vip_data.image_index.unique()),len(vip_data.cell_specimen_id.unique()),110)\n",
        "data_input_n = torch.zeros(len(vip_data.image_index.unique()),len(vip_data.cell_specimen_id.unique()))\n",
        "for i, I in enumerate(vip_data.image_index.unique()):\n",
        "  for c, C in enumerate(vip_data.cell_specimen_id.unique()):\n",
        "    tempset = vip_data[np.logical_and(vip_data.image_index==I,vip_data.cell_specimen_id==C)]\n",
        "    data_input_n[i,c] = len(tempset.cell_mean)\n",
        "    \n",
        "    #print(data_input_n[i,c])\n",
        "    data_input_full[i,c,:(int(data_input_n[i,c]))] = torch.Tensor(tempset.cell_mean.values)\n",
        "\n",
        "test_input = data_input_full[:,:,0]\n",
        "\n",
        "print(test_input.shape)\n",
        "# print(data_input_full.shape)\n",
        "# print(torch.max(data_input_n))\n",
        "\n",
        "def gen_train_samples(N):\n",
        "  num_images = 8\n",
        "  train_input = torch.zeros(len(vip_data.image_index.unique())*N,len(vip_data.cell_specimen_id.unique()))\n",
        "  for n in range(N):   \n",
        "    batch = torch.zeros(len(vip_data.image_index.unique()),len(vip_data.cell_specimen_id.unique()))\n",
        "    for i, I in enumerate(vip_data.image_index.unique()):\n",
        "      for c, C in enumerate(vip_data.cell_specimen_id.unique()):\n",
        "        batch[i,c] = data_input_full[i,c,np.random.choice(int(data_input_n[i,c])-1,1)+1] \n",
        "    train_input[n*num_images:((n+1)*num_images),:] = batch\n",
        "  train_target = torch.Tensor(np.tile(np.eye(8),(N,1)))    \n",
        "  return train_input,train_target\n",
        "\n",
        "train_input, train_target = gen_train_samples(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erjWjBMTPPxD"
      },
      "outputs": [],
      "source": [
        "print(vip_data.columns)\n",
        "print(vip_data[vip_data.exposure_level=='familiar'].mouse_id.unique())\n",
        "print(vip_data[vip_data.exposure_level=='novel'].mouse_id.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42wsoaEb82n-"
      },
      "outputs": [],
      "source": [
        "#Gather data -- full timecourse\n",
        "full_data = vip_data.groupby(['mouse_id','stimulus_presentations_id','cell_specimen_id','image_index'])['trace'].mean().reset_index()\n",
        "full_data = full_data.pivot(index=['mouse_id','stimulus_presentations_id','image_index'],columns='cell_specimen_id',values='trace')\n",
        "full_data = full_data.fillna(0)\n",
        "\n",
        "f1 = full_data.to_numpy()\n",
        "for row in range(len(f1)):\n",
        "  for col in range(len(f1[row])):\n",
        "    if type(f1[row,col]==0)==type(True):\n",
        "      f1[row,col] = np.zeros(85)\n",
        "f1 = np.stack(f1.flatten()).reshape(f1.shape[0],f1.shape[1],85)\n",
        "\n",
        "randsel = np.random.choice(f1.shape[0],int(f1.shape[0]))\n",
        "train_input = f1[randsel[:int(.9*f1.shape[0])],:,:]\n",
        "test_input = f1[randsel[int(.9*f1.shape[0]):],:,:]\n",
        "\n",
        "train_labels = full_data.reset_index().image_index.values[randsel[:int(.9*f1.shape[0])]]\n",
        "train_labels = np.identity(8)[train_labels,]\n",
        "test_labels = full_data.reset_index().image_index.values[randsel[int(.9*f1.shape[0]):]]\n",
        "test_labels = np.identity(8)[test_labels,]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRpTXdAy99pU"
      },
      "outputs": [],
      "source": [
        "#Gather data version 3 -- mean firing rate in window\n",
        "if 'cell_mean' not in vip_data.columns:\n",
        "  vip_data['cell_mean'] = vip_data.apply(lambda x: x.trace[evk_window].mean()-x.trace[bkg_window].mean(),axis=1)\n",
        "\n",
        "full_data = vip_data.groupby(['mouse_id','stimulus_presentations_id','cell_specimen_id','image_index'])['cell_mean'].mean().reset_index()\n",
        "full_data = full_data.pivot(index=['mouse_id','stimulus_presentations_id','image_index'],columns='cell_specimen_id',values='cell_mean')\n",
        "\n",
        "f1 = full_data.values\n",
        "randsel = np.random.choice(f1.shape[0],int(f1.shape[0]))\n",
        "\n",
        "train_input = f1[randsel[:int(.9*f1.shape[0])],]\n",
        "train_input[np.isnan(train_input)] = 0\n",
        "test_input = f1[randsel[int(.9*f1.shape[0]):],]\n",
        "test_input[np.isnan(test_input)] = 0\n",
        "\n",
        "train_labels = full_data.reset_index().image_index.values[randsel[:int(.9*f1.shape[0])]]\n",
        "train_labels = np.identity(8)[train_labels,]\n",
        "test_labels = full_data.reset_index().image_index.values[randsel[int(.9*f1.shape[0]):]]\n",
        "test_labels = np.identity(8)[test_labels,]\n",
        "\n",
        "full_data['mouse_index'] = full_data.groupby('mouse_id', sort=False).ngroup()\n",
        "mouse_index = np.identity(np.max(full_data.mouse_index.values)+1)[full_data.mouse_index.values]\n",
        "train_mouse_index = mouse_index[randsel[:int(.9*f1.shape[0])],]\n",
        "test_mouse_index = mouse_index[randsel[int(.9*f1.shape[0]):],]\n",
        "\n",
        "#train_mouse_id = full_data.reset_index().mouse_id.values[randsel[:int(.9*f1.shape[0])]]\n",
        "#train_mouse_id = np.expand_dims(train_mouse_id,1).astype(float)\n",
        "#test_mouse_id = full_data.reset_index().mouse_id.values[randsel[int(.9*f1.shape[0]):]]\n",
        "#test_mouse_id = np.expand_dims(test_mouse_id,1).astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHG9Rbs5uxbx"
      },
      "outputs": [],
      "source": [
        "fig,axes = plt.subplots(1,3,figsize=(30,5))\n",
        "axes[0].scatter(range(0,len(vip_data.mouse_id)),vip_data.mouse_id)\n",
        "axes[1].scatter(vip_data.cell_specimen_id, vip_data.mouse_id)\n",
        "axes[2].scatter(vip_data.stimulus_presentations_id, vip_data.mouse_id, c=vip_data.image_index)\n",
        "\n",
        "_, cell_mouse = np.unique(vip_data.groupby(by='cell_specimen_id')['mouse_id'].describe()['top'].values,return_inverse='true')\n",
        "print(cell_mouse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoFW7FzW5Jkc"
      },
      "outputs": [],
      "source": [
        "# First net (all cells)\n",
        "net = Net(actv='LeakyReLU(0.1)', input_feature_num=122, hidden_unit_nums=[100, 10, 8], output_feature_num=8).to(DEVICE)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = 'cuda'\n",
        "train_input = torch.Tensor(train_input).to(device)\n",
        "labels = torch.Tensor(train_labels).to(device)\n",
        "test_input = torch.Tensor(test_input).to(device)\n",
        "training_losses = []\n",
        "N_epochs = 2500\n",
        "\n",
        "# Train it\n",
        "net.to(device)\n",
        "net.train()\n",
        "\n",
        "for i in tqdm(range(N_epochs)):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(train_input)\n",
        "  loss = criterion(outputs,labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  training_losses += [loss.item()]\n",
        "\n",
        "net.eval()\n",
        "\n",
        "plt.plot(training_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKvG0j0qW-ac"
      },
      "outputs": [],
      "source": [
        "# MLP with full time course\n",
        "net = Net(actv='LeakyReLU(0.1)', input_feature_num=122*85, hidden_unit_nums=[100, 10, 8], output_feature_num=8).to(DEVICE)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = 'cuda'\n",
        "train_input = torch.Tensor(train_input).to(device).permute(0,2,1).reshape(train_input.shape[0],-1)\n",
        "labels = torch.Tensor(train_labels).to(device)\n",
        "test_input = torch.Tensor(test_input).to(device).permute(0,2,1).reshape(test_input.shape[0],-1)\n",
        "training_losses = []\n",
        "N_epochs = 2500\n",
        "\n",
        "# Train it\n",
        "net.to(device)\n",
        "net.train()\n",
        "\n",
        "for i in tqdm(range(N_epochs)):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(train_input)\n",
        "  loss = criterion(outputs,labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  training_losses += [loss.item()]\n",
        "\n",
        "net.eval()\n",
        "\n",
        "plt.plot(training_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO-oEugaLtOk"
      },
      "outputs": [],
      "source": [
        "#Load big dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dir = '/content/drive/MyDrive/'\n",
        "all_data = pd.read_pickle(dir + 'mice_VIP_N5.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0o1un0YNHno"
      },
      "outputs": [],
      "source": [
        "#Data subsetting\n",
        "all_data = all_data.query('exposure_level==\"familiar\"')\n",
        "all_data = all_data.query('image_name!=\"omitted\"')\n",
        "all_data = all_data.query('is_change==False')\n",
        "\n",
        "#all_data = all_data.groupby(['mouse_id','stimulus_presentations_id','cell_specimen_id','image_name'])['dff_bc'].mean().reset_index()\n",
        "all_data2 = all_data.pivot(index=['mouse_id','stimulus_presentations_id','ophys_session_id','image_name'],columns='cell_specimen_id',values='dff_bc')\n",
        "all_data = all_data2.fillna(0)\n",
        "del all_data2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n64eFIvfgpyA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "net = NetDropout(train_input.shape[1]).to(DEVICE)\n",
        "optimizer = optim.Adam(net.parameters(),weight_decay=1e-5, lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = 'cuda'\n",
        "train_input = torch.Tensor(train_input).to(device)\n",
        "labels = torch.Tensor(train_labels).to(device)\n",
        "test_input = torch.Tensor(test_input).to(device)\n",
        "training_losses = []\n",
        "N_epochs = 5000\n",
        "\n",
        "# Train it\n",
        "net.to(device)\n",
        "net.train()\n",
        "\n",
        "for i in tqdm(range(N_epochs)):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(train_input)\n",
        "  loss = criterion(outputs,labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  \n",
        "\n",
        "  training_losses += [loss.item()]\n",
        "\n",
        "net.eval()\n",
        "\n",
        "plt.plot(training_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5t0sWOIhFCR"
      },
      "outputs": [],
      "source": [
        "#all_data.index.names[0]\n",
        "#all_data.iloc[1]\n",
        "#all_data.loc[467951] #this works, now find all mouse ids\n",
        "\n",
        "import torch.nn.functional as F\n",
        "correct = np.empty(6)\n",
        "correct_train = np.empty(6)\n",
        "\n",
        "mouse_id_list = np.array(list(zip(*all_data.index.values))[0]) #get mouse_ids\n",
        "for m, M in enumerate(np.unique(mouse_id_list)):\n",
        "  \n",
        "  #load subset\n",
        "  this_pt = all_data.loc[M]\n",
        "  f1 = this_pt.to_numpy()\n",
        "  randsel = np.random.choice(f1.shape[0],int(f1.shape[0]))\n",
        "  train_input = f1[randsel[:int(.9*f1.shape[0])],:]\n",
        "  test_input = f1[randsel[int(.9*f1.shape[0]):],:]\n",
        "\n",
        "  unique_label_name, train_labels = np.unique(this_pt.reset_index().image_name.values,return_inverse='true')\n",
        "  train_labels = np.identity(unique_label_name.shape[0])[train_labels[randsel[:int(.9*f1.shape[0])]],]\n",
        "  _, test_labels = np.unique(this_pt.reset_index().image_name.values,return_inverse='true')\n",
        "  test_labels = np.identity(unique_label_name.shape[0])[test_labels[randsel[int(.9*f1.shape[0]):]],]\n",
        "\n",
        "  #train\n",
        "  net = NetDropout(train_input.shape[1]).to(DEVICE)\n",
        "  optimizer = optim.Adam(net.parameters(),weight_decay=1e-5, lr=1e-2)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  device = 'cuda'\n",
        "  train_input = torch.Tensor(train_input).to(device)\n",
        "  labels = torch.Tensor(train_labels).to(device)\n",
        "  test_input = torch.Tensor(test_input).to(device)\n",
        "  training_losses = []\n",
        "  N_epochs = 5000\n",
        "\n",
        "  net.to(device)\n",
        "  net.train()\n",
        "  for i in tqdm(range(N_epochs)):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(train_input)\n",
        "    loss = criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    training_losses += [loss.item()]\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  #testing\n",
        "  outputs = net(test_input).to('cpu')\n",
        "  _, predictedlabels = torch.max(outputs, 1)\n",
        "  _, truelabels = torch.max(torch.tensor(test_labels),1)\n",
        "\n",
        "  outputs_train = net(train_input).to('cpu')\n",
        "  _, predictedlabels_train = torch.max(outputs_train, 1)\n",
        "  _, truelabels_train = torch.max(torch.tensor(train_labels),1)\n",
        "\n",
        "\n",
        "  correct[m] = ((predictedlabels == truelabels).sum().item())/len(truelabels)\n",
        "  correct_train[m] = ((predictedlabels_train == truelabels_train).sum().item())/len(truelabels_train)\n",
        "\n",
        "\n",
        "m += 1\n",
        "  \n",
        "#load subset\n",
        "this_pt = all_data #no subset for the final run through\n",
        "f1 = this_pt.to_numpy()\n",
        "randsel = np.random.choice(f1.shape[0],int(f1.shape[0]))\n",
        "train_input = f1[randsel[:int(.9*f1.shape[0])],:]\n",
        "test_input = f1[randsel[int(.9*f1.shape[0]):],:]\n",
        "\n",
        "unique_label_name, train_labels = np.unique(this_pt.reset_index().image_name.values,return_inverse='true')\n",
        "train_labels = np.identity(unique_label_name.shape[0])[train_labels[randsel[:int(.9*f1.shape[0])]],]\n",
        "_, test_labels = np.unique(this_pt.reset_index().image_name.values,return_inverse='true')\n",
        "test_labels = np.identity(unique_label_name.shape[0])[test_labels[randsel[int(.9*f1.shape[0]):]],]\n",
        "\n",
        "#train\n",
        "net = NetDropout(train_input.shape[1]).to(DEVICE)\n",
        "optimizer = optim.Adam(net.parameters(),weight_decay=1e-5, lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = 'cuda'\n",
        "train_input = torch.Tensor(train_input).to(device)\n",
        "labels = torch.Tensor(train_labels).to(device)\n",
        "test_input = torch.Tensor(test_input).to(device)\n",
        "training_losses = []\n",
        "N_epochs = 5000\n",
        "\n",
        "net.to(device)\n",
        "net.train()\n",
        "for i in tqdm(range(N_epochs)):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(train_input)\n",
        "  loss = criterion(outputs,labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  training_losses += [loss.item()]\n",
        "\n",
        "net.eval()\n",
        "\n",
        "#testing\n",
        "outputs = net(test_input).to('cpu')\n",
        "_, predictedlabels = torch.max(outputs, 1)\n",
        "_, truelabels = torch.max(torch.tensor(test_labels),1)\n",
        "\n",
        "outputs_train = net(train_input).to('cpu')\n",
        "_, predictedlabels_train = torch.max(outputs_train, 1)\n",
        "_, truelabels_train = torch.max(torch.tensor(train_labels),1)\n",
        "\n",
        "correct[m] = ((predictedlabels == truelabels).sum().item())/len(truelabels)\n",
        "correct_train[m] = ((predictedlabels_train == truelabels_train).sum().item())/len(truelabels_train)\n",
        "\n",
        "#final evaluation\n",
        "print(correct)\n",
        "print(correct_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDkA1gtUuJwO"
      },
      "outputs": [],
      "source": [
        "plt.plot(correct)\n",
        "plt.plot(correct_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFBDrSPpvT8p"
      },
      "outputs": [],
      "source": [
        "np.arange(12).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMJ4OO4CSDze"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "f1 = all_data.to_numpy()\n",
        "randsel = np.random.choice(f1.shape[0],int(f1.shape[0]))\n",
        "train_input = f1[randsel[:int(.9*f1.shape[0])],:]\n",
        "test_input = f1[randsel[int(.9*f1.shape[0]):],:]\n",
        "\n",
        "unique_label_name, train_labels = np.unique(all_data.reset_index().image_name.values,return_inverse='true')\n",
        "train_labels = np.identity(unique_label_name.shape[0])[train_labels[randsel[:int(.9*f1.shape[0])]],]\n",
        "_, test_labels = np.unique(all_data.reset_index().image_name.values,return_inverse='true')\n",
        "test_labels = np.identity(unique_label_name.shape[0])[test_labels[randsel[int(.9*f1.shape[0]):]],]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OLB7KCGg-nO"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  \"\"\"\n",
        "  Initialize MLP Network\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, actv, input_feature_num, hidden_unit_nums, output_feature_num,dropout=False):\n",
        "    \"\"\"\n",
        "    Initialize MLP Network parameters\n",
        "\n",
        "    Args:\n",
        "      actv: string\n",
        "        Activation function\n",
        "      input_feature_num: int\n",
        "        Number of input features\n",
        "      hidden_unit_nums: int\n",
        "        Number of units in the hidden layer\n",
        "      output_feature_num: int\n",
        "        Number of output features\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(Net, self).__init__()\n",
        "    self.input_feature_num = input_feature_num # Save the input size for reshaping later\n",
        "    self.mlp = nn.Sequential() # Initialize layers of MLP\n",
        "\n",
        "    in_num = input_feature_num # Initialize the temporary input feature to each layer\n",
        "    for i in range(len(hidden_unit_nums)): # Loop over layers and create each one\n",
        "      \n",
        "      \n",
        "      out_num = hidden_unit_nums[i] # Assign the current layer hidden unit from list\n",
        "      layer = nn.Linear(in_num,out_num) # Use nn.Linear to define the layer\n",
        "      in_num = out_num # Assign next layer input using current layer output\n",
        "      self.mlp.add_module('Linear_%d'%i, layer) # Append layer to the model with a name\n",
        "      if dropout == 1:\n",
        "        if i == 3:\n",
        "          self.mlp.add_module('Dropout_%d'%i, nn.Dropout())\n",
        "\n",
        "      actv_layer = eval('nn.%s'%actv) # Assign activation function (eval allows us to instantiate object from string)\n",
        "      self.mlp.add_module('Activation_%d'%i, actv_layer) # Append activation to the model with a name\n",
        "\n",
        "    out_layer = nn.Linear(in_num, output_feature_num) # Create final layer\n",
        "    self.mlp.add_module('Output_Linear', out_layer) # Append the final layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Simulate forward pass of MLP Network\n",
        "\n",
        "    Args:\n",
        "      x: torch.tensor\n",
        "        Input data\n",
        "\n",
        "    Returns:\n",
        "      logits: Instance of MLP\n",
        "        Forward pass of MLP\n",
        "    \"\"\"\n",
        "    # Reshape inputs to (batch_size, input_feature_num)\n",
        "    # Just in case the input vector is not 2D, like an image!\n",
        "    x = x.view(-1, self.input_feature_num)\n",
        "\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    #raise NotImplementedError(\"Run MLP model\")\n",
        "    ####################################################################\n",
        "\n",
        "    logits = self.mlp(x) # Forward pass of MLP\n",
        "    return logits\n",
        "\n",
        "\n",
        "#input = torch.zeros((100, 2))\n",
        "## Uncomment below to create network and test it on input\n",
        "#net = Net(actv='LeakyReLU(0.1)', input_feature_num=2, hidden_unit_nums=[100, 10, 5], output_feature_num=1).to(DEVICE)\n",
        "#y = net(input.to(DEVICE))\n",
        "#print(f'The output shape is {y.shape} for an input of shape {input.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILtSpXqci0hH"
      },
      "outputs": [],
      "source": [
        "class NetDropout(nn.Module):\n",
        "  \"\"\"\n",
        "  Network Class - 2D with the following structure:\n",
        "  nn.Linear(1, 300) + leaky_relu(self.dropout1(self.fc1(x))) # First fully connected layer with 0.4 dropout\n",
        "  nn.Linear(300, 500) + leaky_relu(self.dropout2(self.fc2(x))) # Second fully connected layer with 0.2 dropout\n",
        "  nn.Linear(500, 1) # Final fully connected layer\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,input_feature_num):\n",
        "    \"\"\"\n",
        "    Initialize parameters of NetDropout\n",
        "\n",
        "    Args:\n",
        "      None\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    super(NetDropout, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(input_feature_num, 300)\n",
        "    self.fc2 = nn.Linear(300, 500)\n",
        "    self.fc3 = nn.Linear(500, 8)\n",
        "    # We add two dropout layers\n",
        "    self.dropout1 = nn.Dropout(0.4)\n",
        "    self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass of NetDropout\n",
        "\n",
        "    Args:\n",
        "      x: torch.tensor\n",
        "        Input features\n",
        "\n",
        "    Returns:\n",
        "      output: torch.tensor\n",
        "        Output/Predictions\n",
        "    \"\"\"\n",
        "    x = F.leaky_relu(self.dropout1(self.fc1(x)))\n",
        "    x = F.leaky_relu(self.dropout2(self.fc2(x)))\n",
        "    output = self.fc3(x)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8EfrFnIjPsH"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "net = NetDropout(train_input.shape[1]).to(DEVICE)\n",
        "optimizer = optim.Adam(net.parameters(),weight_decay=1e-5, lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = 'cuda'\n",
        "train_input = torch.Tensor(train_input).to(device)\n",
        "labels = torch.Tensor(train_labels).to(device)\n",
        "test_input = torch.Tensor(test_input).to(device)\n",
        "training_losses = []\n",
        "N_epochs = 5000\n",
        "\n",
        "# Train it\n",
        "net.to(device)\n",
        "net.train()\n",
        "\n",
        "for i in tqdm(range(N_epochs)):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(train_input)\n",
        "  loss = criterion(outputs,labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  \n",
        "\n",
        "  training_losses += [loss.item()]\n",
        "\n",
        "net.eval()\n",
        "\n",
        "plt.plot(training_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ot4x6FykKE6"
      },
      "outputs": [],
      "source": [
        "train_input.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLBQ7NkWXznu"
      },
      "outputs": [],
      "source": [
        "#Basic net with all_data\n",
        "net = Net(actv='LeakyReLU(0.1)', input_feature_num=train_input.shape[1], hidden_unit_nums=[145, 10, 10, 8], output_feature_num=8,dropout=1).to(DEVICE)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = 'cuda'\n",
        "train_input = torch.Tensor(train_input).to(device)\n",
        "labels = torch.Tensor(train_labels).to(device)\n",
        "test_input = torch.Tensor(test_input).to(device)\n",
        "training_losses = []\n",
        "N_epochs = 10000\n",
        "\n",
        "# Train it\n",
        "net.to(device)\n",
        "net.train()\n",
        "\n",
        "for i in tqdm(range(N_epochs)):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(train_input)\n",
        "  loss = criterion(outputs,labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  \n",
        "\n",
        "  training_losses += [loss.item()]\n",
        "\n",
        "net.eval()\n",
        "\n",
        "plt.plot(training_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiwL-KLgoMaf"
      },
      "outputs": [],
      "source": [
        "net.train()\n",
        "\n",
        "for i in tqdm(range(N_epochs)):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(train_input)\n",
        "  loss = criterion(outputs,labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  \n",
        "\n",
        "  training_losses += [loss.item()]\n",
        "\n",
        "net.eval()\n",
        "\n",
        "plt.plot(training_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVj3aIs0UUNi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHkuBBhUTYUC"
      },
      "outputs": [],
      "source": [
        "# testing full time course\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "#testing\n",
        "outputs = net(test_input).to('cpu')\n",
        "_, predictedlabels = torch.max(outputs, 1)\n",
        "_, truelabels = torch.max(torch.tensor(test_labels),1)\n",
        "\n",
        "outputs_train = net(train_input).to('cpu')\n",
        "_, predictedlabels_train = torch.max(outputs_train, 1)\n",
        "_, truelabels_train = torch.max(torch.tensor(train_labels),1)\n",
        "\n",
        "\n",
        "correct = ((predictedlabels == truelabels).sum().item())/len(truelabels)\n",
        "correct_train = ((predictedlabels_train == truelabels_train).sum().item())/len(truelabels_train)\n",
        "\n",
        "confmat = confusion_matrix(truelabels, predictedlabels)\n",
        "\n",
        "\n",
        "plt.imshow(confmat)\n",
        "print(confmat)\n",
        "print(correct)\n",
        "print(correct_train)\n",
        "# fig,axes = plt.subplots(1,6,figsize=(20,5))\n",
        "# for i in range(0,6):\n",
        "#   #confmat1 = confusion_matrix(predictedlabels[np.where(test_input[:,122+i].to('cpu'))],truelabels[np.where(test_input_mouse[:,122+i].to('cpu'))])\n",
        "#   ConfusionMatrixDisplay.from_predictions(truelabels[np.where(test_input[:,122+i].to('cpu'))],predictedlabels[np.where(test_input_mouse[:,122+i].to('cpu'))],ax=axes[i],normalize='true')\n",
        "\n",
        "# fig,axes = plt.subplots(1,6,figsize=(20,5))\n",
        "# for i in range(0,6):\n",
        "#   #confmat1 = confusion_matrix(predictedlabels[np.where(test_input_mouse[:,122+i].to('cpu'))],truelabels[np.where(test_input_mouse[:,122+i].to('cpu'))])\n",
        "#   ConfusionMatrixDisplay.from_predictions(truelabels_train[np.where(train_input_mouse[:,122+i].to('cpu'))],predictedlabels_train[np.where(train_input_mouse[:,122+i].to('cpu'))],ax=axes[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn8i1ih5d5Tt"
      },
      "outputs": [],
      "source": [
        "# testing full time course\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "#testing\n",
        "outputs = net(test_input).to('cpu')\n",
        "_, predictedlabels = torch.max(outputs, 1)\n",
        "_, truelabels = torch.max(torch.tensor(test_labels),1)\n",
        "\n",
        "outputs_train = net(train_input).to('cpu')\n",
        "_, predictedlabels_train = torch.max(outputs_train, 1)\n",
        "_, truelabels_train = torch.max(torch.tensor(train_labels),1)\n",
        "\n",
        "\n",
        "correct = ((predictedlabels == truelabels).sum().item())/len(truelabels)\n",
        "correct_train = ((predictedlabels_train == truelabels_train).sum().item())/len(truelabels_train)\n",
        "\n",
        "confmat = confusion_matrix(truelabels, predictedlabels)\n",
        "\n",
        "\n",
        "plt.imshow(confmat)\n",
        "print(confmat)\n",
        "print(correct)\n",
        "print(correct_train)\n",
        "# fig,axes = plt.subplots(1,6,figsize=(20,5))\n",
        "# for i in range(0,6):\n",
        "#   #confmat1 = confusion_matrix(predictedlabels[np.where(test_input[:,122+i].to('cpu'))],truelabels[np.where(test_input_mouse[:,122+i].to('cpu'))])\n",
        "#   ConfusionMatrixDisplay.from_predictions(truelabels[np.where(test_input[:,122+i].to('cpu'))],predictedlabels[np.where(test_input_mouse[:,122+i].to('cpu'))],ax=axes[i],normalize='true')\n",
        "\n",
        "# fig,axes = plt.subplots(1,6,figsize=(20,5))\n",
        "# for i in range(0,6):\n",
        "#   #confmat1 = confusion_matrix(predictedlabels[np.where(test_input_mouse[:,122+i].to('cpu'))],truelabels[np.where(test_input_mouse[:,122+i].to('cpu'))])\n",
        "#   ConfusionMatrixDisplay.from_predictions(truelabels_train[np.where(train_input_mouse[:,122+i].to('cpu'))],predictedlabels_train[np.where(train_input_mouse[:,122+i].to('cpu'))],ax=axes[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eUJYwp_FXSw"
      },
      "outputs": [],
      "source": [
        "#NET with mouse ID\n",
        "net = Net(actv='LeakyReLU(0.1)', input_feature_num=128, hidden_unit_nums=[100, 10, 8], output_feature_num=8).to(DEVICE)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = 'cuda'\n",
        "\n",
        "train_input_mouse = np.hstack((train_input, train_mouse_index))\n",
        "train_input_mouse = torch.Tensor(train_input_mouse).to(device)\n",
        "\n",
        "labels = torch.Tensor(train_labels).to(device)\n",
        "\n",
        "test_input_mouse = np.hstack((test_input, test_mouse_index))\n",
        "test_input_mouse = torch.Tensor(test_input_mouse).to(device)\n",
        "\n",
        "\n",
        "training_losses = []\n",
        "N_epochs = 4500\n",
        "\n",
        "# Train it\n",
        "net.to(device)\n",
        "net.train()\n",
        "\n",
        "for i in tqdm(range(N_epochs)):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  #randomly shuffle cell\n",
        "  #train_input_sh = train_input_mouse[:,np.hstack((np.random.choice(122,122), np.arange(122,128)))]\n",
        "\n",
        "  outputs = net(train_input_mouse)\n",
        "  loss = criterion(outputs,labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  training_losses += [loss.item()]\n",
        "\n",
        "net.eval()\n",
        "\n",
        "plt.plot(training_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2KNKvfONlYZ"
      },
      "outputs": [],
      "source": [
        "#NET with mouse ID and full time course\n",
        "net = Net(actv='LeakyReLU(0.1)', input_feature_num=[128, 85], hidden_unit_nums=[100, 10, 8], output_feature_num=8).to(DEVICE)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = 'cuda'\n",
        "\n",
        "train_input_mouse = np.hstack((train_input, train_mouse_index))\n",
        "train_input_mouse = torch.Tensor(train_input_mouse).to(device)\n",
        "\n",
        "labels = torch.Tensor(train_labels).to(device)\n",
        "\n",
        "test_input_mouse = np.hstack((test_input, test_mouse_index))\n",
        "test_input_mouse = torch.Tensor(test_input_mouse).to(device)\n",
        "\n",
        "\n",
        "training_losses = []\n",
        "N_epochs = 4500\n",
        "\n",
        "# Train it\n",
        "net.to(device)\n",
        "net.train()\n",
        "\n",
        "for i in tqdm(range(N_epochs)):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  #randomly shuffle cell\n",
        "  #train_input_sh = train_input_mouse[:,np.hstack((np.random.choice(122,122), np.arange(122,128)))]\n",
        "\n",
        "  outputs = net(train_input_mouse)\n",
        "  loss = criterion(outputs,labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  training_losses += [loss.item()]\n",
        "\n",
        "net.eval()\n",
        "\n",
        "plt.plot(training_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfI50nsE5tFe"
      },
      "outputs": [],
      "source": [
        "np.hstack((np.random.choice(122,122), np.arange(122,128)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcSt7qdeD1EI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "#testing\n",
        "outputs = net(test_input_mouse).to('cpu')\n",
        "_, predictedlabels = torch.max(outputs, 1)\n",
        "_, truelabels = torch.max(torch.tensor(test_labels),1)\n",
        "\n",
        "outputs_train = net(train_input_mouse).to('cpu')\n",
        "_, predictedlabels_train = torch.max(outputs_train, 1)\n",
        "_, truelabels_train = torch.max(torch.tensor(train_labels),1)\n",
        "\n",
        "\n",
        "correct = ((predictedlabels == truelabels).sum().item())/len(truelabels)\n",
        "\n",
        "confmat = confusion_matrix(truelabels, predictedlabels)\n",
        "\n",
        "#plt.imshow(confmat)\n",
        "#print(confmat)\n",
        "#print(correct)\n",
        "\n",
        "fig,axes = plt.subplots(1,6,figsize=(20,5))\n",
        "for i in range(0,6):\n",
        "  confmat1 = confusion_matrix(predictedlabels[np.where(test_input_mouse[:,122+i].to('cpu'))],truelabels[np.where(test_input_mouse[:,122+i].to('cpu'))])\n",
        "  ConfusionMatrixDisplay.from_predictions(truelabels[np.where(test_input_mouse[:,122+i].to('cpu'))],predictedlabels[np.where(test_input_mouse[:,122+i].to('cpu'))],ax=axes[i],normalize='true')\n",
        "\n",
        "fig,axes = plt.subplots(1,6,figsize=(20,5))\n",
        "for i in range(0,6):\n",
        "  #confmat1 = confusion_matrix(predictedlabels[np.where(test_input_mouse[:,122+i].to('cpu'))],truelabels[np.where(test_input_mouse[:,122+i].to('cpu'))])\n",
        "  ConfusionMatrixDisplay.from_predictions(truelabels_train[np.where(train_input_mouse[:,122+i].to('cpu'))],predictedlabels_train[np.where(train_input_mouse[:,122+i].to('cpu'))],ax=axes[i])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MaXQg0JDfMN"
      },
      "outputs": [],
      "source": [
        "# testing full time course\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "#testing\n",
        "outputs = net(test_input).to('cpu')\n",
        "_, predictedlabels = torch.max(outputs, 1)\n",
        "_, truelabels = torch.max(torch.tensor(test_labels),1)\n",
        "\n",
        "outputs_train = net(train_input).to('cpu')\n",
        "_, predictedlabels_train = torch.max(outputs_train, 1)\n",
        "_, truelabels_train = torch.max(torch.tensor(train_labels),1)\n",
        "\n",
        "\n",
        "correct = ((predictedlabels == truelabels).sum().item())/len(truelabels)\n",
        "\n",
        "confmat = confusion_matrix(truelabels, predictedlabels)\n",
        "\n",
        "plt.imshow(confmat)\n",
        "print(confmat)\n",
        "print(correct)\n",
        "\n",
        "# fig,axes = plt.subplots(1,6,figsize=(20,5))\n",
        "# for i in range(0,6):\n",
        "#   #confmat1 = confusion_matrix(predictedlabels[np.where(test_input[:,122+i].to('cpu'))],truelabels[np.where(test_input_mouse[:,122+i].to('cpu'))])\n",
        "#   ConfusionMatrixDisplay.from_predictions(truelabels[np.where(test_input[:,122+i].to('cpu'))],predictedlabels[np.where(test_input_mouse[:,122+i].to('cpu'))],ax=axes[i],normalize='true')\n",
        "\n",
        "# fig,axes = plt.subplots(1,6,figsize=(20,5))\n",
        "# for i in range(0,6):\n",
        "#   #confmat1 = confusion_matrix(predictedlabels[np.where(test_input_mouse[:,122+i].to('cpu'))],truelabels[np.where(test_input_mouse[:,122+i].to('cpu'))])\n",
        "#   ConfusionMatrixDisplay.from_predictions(truelabels_train[np.where(train_input_mouse[:,122+i].to('cpu'))],predictedlabels_train[np.where(train_input_mouse[:,122+i].to('cpu'))],ax=axes[i])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAZnGxxA4R_-"
      },
      "outputs": [],
      "source": [
        "torch.sum(train_input_mouse!=0,axis=1).max()-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV7jijIfGC6R"
      },
      "outputs": [],
      "source": [
        "def net_svd(model, in_dim):\n",
        "  \"\"\"\n",
        "  Performs a Singular Value Decomposition on a given model weights\n",
        "  Args:\n",
        "    model: torch.nn.Module\n",
        "      Neural network model\n",
        "    in_dim: int\n",
        "      The input dimension of the model\n",
        "  Returns:\n",
        "    U: torch.tensor\n",
        "      Orthogonal matrix\n",
        "    Σ: torch.tensor\n",
        "      Diagonal matrix\n",
        "    V: torch.tensor\n",
        "      Orthogonal matrix\n",
        "  \"\"\"\n",
        "  W_tot = torch.eye(in_dim)\n",
        "  for weight in model.parameters():\n",
        "    W_tot = weight @ W_tot\n",
        "  U, Σ, V = torch.svd(W_tot)\n",
        "  return U, Σ, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ0RoVK88JfO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "weights = net.mlp[6].weight@net.mlp[4].weight@net.mlp[2].weight@net.mlp[0].weight\n",
        "#weights = net.mlp[0].weight\n",
        "\n",
        "weights = weights.to('cpu').detach().numpy()\n",
        "\n",
        "print(cell_mouse)\n",
        "weights_plus_mouse = np.vstack((weights[:,:122], cell_mouse,cell_mouse,cell_mouse))\n",
        "\n",
        "plt.imshow(np.abs(weights),aspect='auto')\n",
        "\n",
        "U, Sig, V = torch.svd(torch.tensor(weights))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Sig)\n",
        "\n",
        "\n",
        "Vweights_plus_mouse = np.vstack((V[:122,:3].T, cell_mouse/10))\n",
        "plt.figure()\n",
        "fig2 = plt.imshow(Vweights_plus_mouse,aspect='auto')\n",
        "#U, Sigma, V = net_svd(net.to('cpu'), train_input_mouse.size()[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWsgU1bjdw9i"
      },
      "outputs": [],
      "source": [
        "def train_test_classification(net, criterion, optimizer, train_loader,\n",
        "                              test_loader, num_epochs=1, verbose=True,\n",
        "                              training_plot=False, device='cpu'):\n",
        "  \"\"\"\n",
        "  Accumulate training loss/Evaluate performance\n",
        "\n",
        "  Args:\n",
        "    net: instance of Net class\n",
        "      Describes the model with ReLU activation, batch size 128\n",
        "    criterion: torch.nn type\n",
        "      Criterion combines LogSoftmax and NLLLoss in one single class.\n",
        "    optimizer: torch.optim type\n",
        "      Implements Adam algorithm.\n",
        "    train_loader: torch.utils.data type\n",
        "      Combines the train dataset and sampler, and provides an iterable over the given dataset.\n",
        "    test_loader: torch.utils.data type\n",
        "      Combines the test dataset and sampler, and provides an iterable over the given dataset.\n",
        "    num_epochs: int\n",
        "      Number of epochs [default: 1]\n",
        "    verbose: boolean\n",
        "      If True, print statistics\n",
        "    training_plot=False\n",
        "      If True, display training plot\n",
        "    device: string\n",
        "      CUDA/GPU if available, CPU otherwise\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  net.train()\n",
        "  training_losses = []\n",
        "  for epoch in tqdm(range(num_epochs)):  # Loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      # Get the inputs; data is a list of [inputs, labels]\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.to(device).float()\n",
        "      labels = labels.to(device).long()\n",
        "\n",
        "      # Zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print statistics\n",
        "      if verbose:\n",
        "        training_losses += [loss.item()]\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  def test(data_loader):\n",
        "    \"\"\"\n",
        "    Function to gauge network performance\n",
        "\n",
        "    Args:\n",
        "      data_loader: torch.utils.data type\n",
        "      Combines the test dataset and sampler, and provides an iterable over the given dataset.\n",
        "\n",
        "    Returns:\n",
        "      acc: float\n",
        "        Performance of the network\n",
        "      total: int\n",
        "        Number of datapoints in the dataloader\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in data_loader:\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.to(device).float()\n",
        "      labels = labels.to(device).long()\n",
        "\n",
        "      outputs = net(inputs)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    return total, acc\n",
        "\n",
        "  train_total, train_acc = test(train_loader)\n",
        "  test_total, test_acc = test(test_loader)\n",
        "\n",
        "  if verbose:\n",
        "    print(f\"Accuracy on the {train_total} training samples: {train_acc:0.2f}\")\n",
        "    print(f\"Accuracy on the {test_total} testing samples: {test_acc:0.2f}\")\n",
        "\n",
        "  if training_plot:\n",
        "    plt.plot(training_losses)\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Training loss')\n",
        "    plt.show()\n",
        "\n",
        "  return train_acc, test_acc"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Saltenas.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('mindscope_utilities')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c2d75289377afe78148052a4afb146342057af0f156c5d68e5a1810c42f31f9e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
